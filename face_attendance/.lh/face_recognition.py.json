{
    "sourceFile": "face_recognition.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1686869881641,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1686882736810,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,241 +1,241 @@\n-# -*- coding: utf-8 -*-\r\n+# # -*- coding: utf-8 -*-\r\n \r\n-import math\r\n-import PIL.Image\r\n-import dlib\r\n-import numpy as np\r\n-from PIL import ImageFile\r\n+# import math\r\n+# import PIL.Image\r\n+# import dlib\r\n+# import numpy as np\r\n+# from PIL import ImageFile\r\n \r\n-try:\r\n-    import face_recognition_models\r\n-except Exception:\r\n-    print(\"Please install `face_recognition_models` with this command before using `face_recognition`:\\n\")\r\n-    print(\"pip install git+https://github.com/ageitgey/face_recognition_models\")\r\n-    quit()\r\n+# try:\r\n+#     import face_recognition_models\r\n+# except Exception:\r\n+#     print(\"Please install `face_recognition_models` with this command before using `face_recognition`:\\n\")\r\n+#     print(\"pip install git+https://github.com/ageitgey/face_recognition_models\")\r\n+#     quit()\r\n \r\n-ImageFile.LOAD_TRUNCATED_IMAGES = True\r\n+# ImageFile.LOAD_TRUNCATED_IMAGES = True\r\n \r\n-face_detector = dlib.get_frontal_face_detector()\r\n+# face_detector = dlib.get_frontal_face_detector()\r\n \r\n-predictor_68_point_model = face_recognition_models.pose_predictor_model_location()\r\n-pose_predictor_68_point = dlib.shape_predictor(predictor_68_point_model)\r\n+# predictor_68_point_model = face_recognition_models.pose_predictor_model_location()\r\n+# pose_predictor_68_point = dlib.shape_predictor(predictor_68_point_model)\r\n \r\n-predictor_5_point_model = face_recognition_models.pose_predictor_five_point_model_location()\r\n-pose_predictor_5_point = dlib.shape_predictor(predictor_5_point_model)\r\n+# predictor_5_point_model = face_recognition_models.pose_predictor_five_point_model_location()\r\n+# pose_predictor_5_point = dlib.shape_predictor(predictor_5_point_model)\r\n \r\n-cnn_face_detection_model = face_recognition_models.cnn_face_detector_model_location()\r\n-cnn_face_detector = dlib.cnn_face_detection_model_v1(cnn_face_detection_model)\r\n+# cnn_face_detection_model = face_recognition_models.cnn_face_detector_model_location()\r\n+# cnn_face_detector = dlib.cnn_face_detection_model_v1(cnn_face_detection_model)\r\n \r\n-face_recognition_model = face_recognition_models.face_recognition_model_location()\r\n-face_encoder = dlib.face_recognition_model_v1(face_recognition_model)\r\n+# face_recognition_model = face_recognition_models.face_recognition_model_location()\r\n+# face_encoder = dlib.face_recognition_model_v1(face_recognition_model)\r\n \r\n \r\n-def _rect_to_css(rect):\r\n-    \"\"\"\r\n-    Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order\r\n+# def _rect_to_css(rect):\r\n+#     \"\"\"\r\n+#     Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order\r\n \r\n-    :param rect: a dlib 'rect' object\r\n-    :return: a plain tuple representation of the rect in (top, right, bottom, left) order\r\n-    \"\"\"\r\n-    return rect.top(), rect.right(), rect.bottom(), rect.left()\r\n+#     :param rect: a dlib 'rect' object\r\n+#     :return: a plain tuple representation of the rect in (top, right, bottom, left) order\r\n+#     \"\"\"\r\n+#     return rect.top(), rect.right(), rect.bottom(), rect.left()\r\n \r\n \r\n-def _css_to_rect(css):\r\n-    \"\"\"\r\n-    Convert a tuple in (top, right, bottom, left) order to a dlib `rect` object\r\n+# def _css_to_rect(css):\r\n+#     \"\"\"\r\n+#     Convert a tuple in (top, right, bottom, left) order to a dlib `rect` object\r\n \r\n-    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\r\n-    :return: a dlib `rect` object\r\n-    \"\"\"\r\n-    return dlib.rectangle(css[3], css[0], css[1], css[2])\r\n+#     :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\r\n+#     :return: a dlib `rect` object\r\n+#     \"\"\"\r\n+#     return dlib.rectangle(css[3], css[0], css[1], css[2])\r\n \r\n \r\n-def _trim_css_to_bounds(css, image_shape):\r\n-    \"\"\"\r\n-    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\r\n+# def _trim_css_to_bounds(css, image_shape):\r\n+#     \"\"\"\r\n+#     Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\r\n \r\n-    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\r\n-    :param image_shape: numpy shape of the image array\r\n-    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order\r\n-    \"\"\"\r\n-    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)\r\n+#     :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\r\n+#     :param image_shape: numpy shape of the image array\r\n+#     :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order\r\n+#     \"\"\"\r\n+#     return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)\r\n \r\n \r\n-def face_distance(face_encodings, face_to_compare):\r\n-    \"\"\"\r\n-    Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\r\n-    for each comparison face. The distance tells you how similar the faces are.\r\n+# def face_distance(face_encodings, face_to_compare):\r\n+#     \"\"\"\r\n+#     Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\r\n+#     for each comparison face. The distance tells you how similar the faces are.\r\n \r\n-    :param faces: List of face encodings to compare\r\n-    :param face_to_compare: A face encoding to compare against\r\n-    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array\r\n-    \"\"\"\r\n-    if len(face_encodings) == 0:\r\n-        return np.empty((0))\r\n+#     :param faces: List of face encodings to compare\r\n+#     :param face_to_compare: A face encoding to compare against\r\n+#     :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array\r\n+#     \"\"\"\r\n+#     if len(face_encodings) == 0:\r\n+#         return np.empty((0))\r\n \r\n-    return np.linalg.norm(face_encodings - face_to_compare, axis=1)\r\n+#     return np.linalg.norm(face_encodings - face_to_compare, axis=1)\r\n \r\n \r\n-def load_image_file(file, mode='RGB'):\r\n-    \"\"\"\r\n-    Loads an image file (.jpg, .png, etc) into a numpy array\r\n+# def load_image_file(file, mode='RGB'):\r\n+#     \"\"\"\r\n+#     Loads an image file (.jpg, .png, etc) into a numpy array\r\n \r\n-    :param file: image file name or file object to load\r\n-    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.\r\n-    :return: image contents as numpy array\r\n-    \"\"\"\r\n-    im = PIL.Image.open(file)\r\n-    if mode:\r\n-        im = im.convert(mode)\r\n-    return np.array(im)\r\n+#     :param file: image file name or file object to load\r\n+#     :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.\r\n+#     :return: image contents as numpy array\r\n+#     \"\"\"\r\n+#     im = PIL.Image.open(file)\r\n+#     if mode:\r\n+#         im = im.convert(mode)\r\n+#     return np.array(im)\r\n \r\n \r\n-def _raw_face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\r\n-    \"\"\"\r\n-    Returns an array of bounding boxes of human faces in a image\r\n+# def _raw_face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\r\n+#     \"\"\"\r\n+#     Returns an array of bounding boxes of human faces in a image\r\n \r\n-    :param img: An image (as a numpy array)\r\n-    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\r\n-    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\r\n-                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\r\n-    :return: A list of dlib 'rect' objects of found face locations\r\n-    \"\"\"\r\n-    if model == \"cnn\":\r\n-        return cnn_face_detector(img, number_of_times_to_upsample)\r\n-    else:\r\n-        return face_detector(img, number_of_times_to_upsample)\r\n+#     :param img: An image (as a numpy array)\r\n+#     :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\r\n+#     :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\r\n+#                   deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\r\n+#     :return: A list of dlib 'rect' objects of found face locations\r\n+#     \"\"\"\r\n+#     if model == \"cnn\":\r\n+#         return cnn_face_detector(img, number_of_times_to_upsample)\r\n+#     else:\r\n+#         return face_detector(img, number_of_times_to_upsample)\r\n \r\n \r\n-def face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\r\n-    \"\"\"\r\n-    Returns an array of bounding boxes of human faces in a image\r\n+# def face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\r\n+#     \"\"\"\r\n+#     Returns an array of bounding boxes of human faces in a image\r\n \r\n-    :param img: An image (as a numpy array)\r\n-    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\r\n-    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\r\n-                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\r\n-    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\r\n-    \"\"\"\r\n-    if model == \"cnn\":\r\n-        return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, \"cnn\")]\r\n-    else:\r\n-        return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]\r\n+#     :param img: An image (as a numpy array)\r\n+#     :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\r\n+#     :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\r\n+#                   deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\r\n+#     :return: A list of tuples of found face locations in css (top, right, bottom, left) order\r\n+#     \"\"\"\r\n+#     if model == \"cnn\":\r\n+#         return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, \"cnn\")]\r\n+#     else:\r\n+#         return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]\r\n \r\n \r\n-def _raw_face_locations_batched(images, number_of_times_to_upsample=1, batch_size=128):\r\n-    \"\"\"\r\n-    Returns an 2d array of dlib rects of human faces in a image using the cnn face detector\r\n+# def _raw_face_locations_batched(images, number_of_times_to_upsample=1, batch_size=128):\r\n+#     \"\"\"\r\n+#     Returns an 2d array of dlib rects of human faces in a image using the cnn face detector\r\n \r\n-    :param img: A list of images (each as a numpy array)\r\n-    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\r\n-    :return: A list of dlib 'rect' objects of found face locations\r\n-    \"\"\"\r\n-    return cnn_face_detector(images, number_of_times_to_upsample, batch_size=batch_size)\r\n+#     :param img: A list of images (each as a numpy array)\r\n+#     :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\r\n+#     :return: A list of dlib 'rect' objects of found face locations\r\n+#     \"\"\"\r\n+#     return cnn_face_detector(images, number_of_times_to_upsample, batch_size=batch_size)\r\n \r\n \r\n-def batch_face_locations(images, number_of_times_to_upsample=1, batch_size=128):\r\n-    \"\"\"\r\n-    Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector\r\n-    If you are using a GPU, this can give you much faster results since the GPU\r\n-    can process batches of images at once. If you aren't using a GPU, you don't need this function.\r\n+# def batch_face_locations(images, number_of_times_to_upsample=1, batch_size=128):\r\n+#     \"\"\"\r\n+#     Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector\r\n+#     If you are using a GPU, this can give you much faster results since the GPU\r\n+#     can process batches of images at once. If you aren't using a GPU, you don't need this function.\r\n \r\n-    :param images: A list of images (each as a numpy array)\r\n-    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\r\n-    :param batch_size: How many images to include in each GPU processing batch.\r\n-    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\r\n-    \"\"\"\r\n-    def convert_cnn_detections_to_css(detections):\r\n-        return [_trim_css_to_bounds(_rect_to_css(face.rect), images[0].shape) for face in detections]\r\n+#     :param images: A list of images (each as a numpy array)\r\n+#     :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\r\n+#     :param batch_size: How many images to include in each GPU processing batch.\r\n+#     :return: A list of tuples of found face locations in css (top, right, bottom, left) order\r\n+#     \"\"\"\r\n+#     def convert_cnn_detections_to_css(detections):\r\n+#         return [_trim_css_to_bounds(_rect_to_css(face.rect), images[0].shape) for face in detections]\r\n \r\n-    raw_detections_batched = _raw_face_locations_batched(images, number_of_times_to_upsample, batch_size)\r\n+#     raw_detections_batched = _raw_face_locations_batched(images, number_of_times_to_upsample, batch_size)\r\n \r\n-    return list(map(convert_cnn_detections_to_css, raw_detections_batched))\r\n+#     return list(map(convert_cnn_detections_to_css, raw_detections_batched))\r\n \r\n \r\n-def _raw_face_landmarks(face_image, face_locations=None, model=\"large\"):\r\n-    if face_locations is None:\r\n-        face_locations = _raw_face_locations(face_image)\r\n-    else:\r\n-        face_locations = [_css_to_rect(face_location) for face_location in face_locations]\r\n+# def _raw_face_landmarks(face_image, face_locations=None, model=\"large\"):\r\n+#     if face_locations is None:\r\n+#         face_locations = _raw_face_locations(face_image)\r\n+#     else:\r\n+#         face_locations = [_css_to_rect(face_location) for face_location in face_locations]\r\n \r\n-    pose_predictor = pose_predictor_68_point\r\n+#     pose_predictor = pose_predictor_68_point\r\n \r\n-    if model == \"small\":\r\n-        pose_predictor = pose_predictor_5_point\r\n+#     if model == \"small\":\r\n+#         pose_predictor = pose_predictor_5_point\r\n \r\n-    return [pose_predictor(face_image, face_location) for face_location in face_locations]\r\n+#     return [pose_predictor(face_image, face_location) for face_location in face_locations]\r\n \r\n \r\n-def face_landmarks(face_image, face_locations=None, model=\"large\"):\r\n-    \"\"\"\r\n-    Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image\r\n+# def face_landmarks(face_image, face_locations=None, model=\"large\"):\r\n+#     \"\"\"\r\n+#     Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image\r\n \r\n-    :param face_image: image to search\r\n-    :param face_locations: Optionally provide a list of face locations to check.\r\n-    :param model: Optional - which model to use. \"large\" (default) or \"small\" which only returns 5 points but is faster.\r\n-    :return: A list of dicts of face feature locations (eyes, nose, etc)\r\n-    \"\"\"\r\n-    landmarks = _raw_face_landmarks(face_image, face_locations, model)\r\n-    landmarks_as_tuples = [[(p.x, p.y) for p in landmark.parts()] for landmark in landmarks]\r\n+#     :param face_image: image to search\r\n+#     :param face_locations: Optionally provide a list of face locations to check.\r\n+#     :param model: Optional - which model to use. \"large\" (default) or \"small\" which only returns 5 points but is faster.\r\n+#     :return: A list of dicts of face feature locations (eyes, nose, etc)\r\n+#     \"\"\"\r\n+#     landmarks = _raw_face_landmarks(face_image, face_locations, model)\r\n+#     landmarks_as_tuples = [[(p.x, p.y) for p in landmark.parts()] for landmark in landmarks]\r\n \r\n-    # For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png\r\n-    if model == 'large':\r\n-        landmarks = [{\r\n-            \"chin\": points[0:17],\r\n-            \"left_eyebrow\": points[17:22],\r\n-            \"right_eyebrow\": points[22:27],\r\n-            \"nose_bridge\": points[27:31],\r\n-            \"nose_tip\": points[31:36],\r\n-            \"left_eye\": points[36:42],\r\n-            \"right_eye\": points[42:48],\r\n-            \"top_lip\": points[48:55] + [points[64]] + [points[63]] + [points[62]] + [points[61]] + [points[60]],\r\n-            \"bottom_lip\": points[54:60] + [points[48]] + [points[60]] + [points[67]] + [points[66]] + [points[65]] + [points[64]]\r\n-        } for points in landmarks_as_tuples]\r\n+#     # For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png\r\n+#     if model == 'large':\r\n+#         landmarks = [{\r\n+#             \"chin\": points[0:17],\r\n+#             \"left_eyebrow\": points[17:22],\r\n+#             \"right_eyebrow\": points[22:27],\r\n+#             \"nose_bridge\": points[27:31],\r\n+#             \"nose_tip\": points[31:36],\r\n+#             \"left_eye\": points[36:42],\r\n+#             \"right_eye\": points[42:48],\r\n+#             \"top_lip\": points[48:55] + [points[64]] + [points[63]] + [points[62]] + [points[61]] + [points[60]],\r\n+#             \"bottom_lip\": points[54:60] + [points[48]] + [points[60]] + [points[67]] + [points[66]] + [points[65]] + [points[64]]\r\n+#         } for points in landmarks_as_tuples]\r\n         \r\n-        for landmark in landmarks:\r\n-            points = landmark['top_lip']\r\n-            # Dapatkan jarak antara kedua mata\r\n-            jarak_mata = math.sqrt((landmark['left_eye'][1][0] - landmark['right_eye'][0][0])**2 + (landmark['left_eye'][1][1] - landmark['right_eye'][0][1])**2)\r\n+#         for landmark in landmarks:\r\n+#             points = landmark['top_lip']\r\n+#             # Dapatkan jarak antara kedua mata\r\n+#             jarak_mata = math.sqrt((landmark['left_eye'][1][0] - landmark['right_eye'][0][0])**2 + (landmark['left_eye'][1][1] - landmark['right_eye'][0][1])**2)\r\n \r\n-            # Sesuaikan titik-titik untuk bibir bagian atas\r\n-            points[0][0] = landmark['nose_tip'][0][0] - jarak_mata * 0.1\r\n-            points[0][1] = landmark['nose_tip'][0][1] - jarak_mata * 0.1\r\n+#             # Sesuaikan titik-titik untuk bibir bagian atas\r\n+#             points[0][0] = landmark['nose_tip'][0][0] - jarak_mata * 0.1\r\n+#             points[0][1] = landmark['nose_tip'][0][1] - jarak_mata * 0.1\r\n \r\n-            # Sesuaikan titik-titik untuk bibir bagian bawah\r\n-            points[6][0] = landmark['nose_tip'][0][0] + jarak_mata * 0.1\r\n-            points[6][1] = landmark['nose_tip'][0][1] + jarak_mata * 0.1\r\n-    elif model == 'small':\r\n-        landmarks = [{\r\n-            \"nose_tip\": [points[4]],\r\n-            \"left_eye\": points[2:4],\r\n-            \"right_eye\": points[0:2],\r\n-        } for points in landmarks_as_tuples]\r\n-    else:\r\n-        raise ValueError(\"Invalid landmarks model type. Supported models are ['small', 'large'].\")\r\n+#             # Sesuaikan titik-titik untuk bibir bagian bawah\r\n+#             points[6][0] = landmark['nose_tip'][0][0] + jarak_mata * 0.1\r\n+#             points[6][1] = landmark['nose_tip'][0][1] + jarak_mata * 0.1\r\n+#     elif model == 'small':\r\n+#         landmarks = [{\r\n+#             \"nose_tip\": [points[4]],\r\n+#             \"left_eye\": points[2:4],\r\n+#             \"right_eye\": points[0:2],\r\n+#         } for points in landmarks_as_tuples]\r\n+#     else:\r\n+#         raise ValueError(\"Invalid landmarks model type. Supported models are ['small', 'large'].\")\r\n \r\n \r\n \r\n-def face_encodings(face_image, known_face_locations=None, num_jitters=2, model=\"large\"):\r\n-    \"\"\"\r\n-    Given an image, return the 128-dimension face encoding for each face in the image.\r\n+# def face_encodings(face_image, known_face_locations=None, num_jitters=2, model=\"large\"):\r\n+#     \"\"\"\r\n+#     Given an image, return the 128-dimension face encoding for each face in the image.\r\n \r\n-    :param face_image: The image that contains one or more faces\r\n-    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.\r\n-    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)\r\n-    :param model: Optional - which model to use. \"large\" (default) or \"small\" which only returns 5 points but is faster.\r\n-    :return: A list of 128-dimensional face encodings (one for each face in the image)\r\n-    \"\"\"\r\n-    raw_landmarks = _raw_face_landmarks(face_image, known_face_locations, model)\r\n-    return [np.array(face_encoder.compute_face_descriptor(face_image, raw_landmark_set, num_jitters)) for raw_landmark_set in raw_landmarks]\r\n+#     :param face_image: The image that contains one or more faces\r\n+#     :param known_face_locations: Optional - the bounding boxes of each face if you already know them.\r\n+#     :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)\r\n+#     :param model: Optional - which model to use. \"large\" (default) or \"small\" which only returns 5 points but is faster.\r\n+#     :return: A list of 128-dimensional face encodings (one for each face in the image)\r\n+#     \"\"\"\r\n+#     raw_landmarks = _raw_face_landmarks(face_image, known_face_locations, model)\r\n+#     return [np.array(face_encoder.compute_face_descriptor(face_image, raw_landmark_set, num_jitters)) for raw_landmark_set in raw_landmarks]\r\n \r\n \r\n-def compare_faces(known_face_encodings, face_encoding_to_check, tolerance=0.6):\r\n-    \"\"\"\r\n-    Compare a list of face encodings against a candidate encoding to see if they match.\r\n+# def compare_faces(known_face_encodings, face_encoding_to_check, tolerance=0.6):\r\n+#     \"\"\"\r\n+#     Compare a list of face encodings against a candidate encoding to see if they match.\r\n \r\n-    :param known_face_encodings: A list of known face encodings\r\n-    :param face_encoding_to_check: A single face encoding to compare against the list\r\n-    :param tolerance: How much distance between faces to consider it a match. Lower is more strict. 0.6 is typical best performance.\r\n-    :return: A list of True/False values indicating which known_face_encodings match the face encoding to check\r\n-    \"\"\"\r\n-    return list(face_distance(known_face_encodings, face_encoding_to_check) <= tolerance)\r\n+#     :param known_face_encodings: A list of known face encodings\r\n+#     :param face_encoding_to_check: A single face encoding to compare against the list\r\n+#     :param tolerance: How much distance between faces to consider it a match. Lower is more strict. 0.6 is typical best performance.\r\n+#     :return: A list of True/False values indicating which known_face_encodings match the face encoding to check\r\n+#     \"\"\"\r\n+#     return list(face_distance(known_face_encodings, face_encoding_to_check) <= tolerance)\r\n"
                }
            ],
            "date": 1686869881641,
            "name": "Commit-0",
            "content": "# -*- coding: utf-8 -*-\r\n\r\nimport math\r\nimport PIL.Image\r\nimport dlib\r\nimport numpy as np\r\nfrom PIL import ImageFile\r\n\r\ntry:\r\n    import face_recognition_models\r\nexcept Exception:\r\n    print(\"Please install `face_recognition_models` with this command before using `face_recognition`:\\n\")\r\n    print(\"pip install git+https://github.com/ageitgey/face_recognition_models\")\r\n    quit()\r\n\r\nImageFile.LOAD_TRUNCATED_IMAGES = True\r\n\r\nface_detector = dlib.get_frontal_face_detector()\r\n\r\npredictor_68_point_model = face_recognition_models.pose_predictor_model_location()\r\npose_predictor_68_point = dlib.shape_predictor(predictor_68_point_model)\r\n\r\npredictor_5_point_model = face_recognition_models.pose_predictor_five_point_model_location()\r\npose_predictor_5_point = dlib.shape_predictor(predictor_5_point_model)\r\n\r\ncnn_face_detection_model = face_recognition_models.cnn_face_detector_model_location()\r\ncnn_face_detector = dlib.cnn_face_detection_model_v1(cnn_face_detection_model)\r\n\r\nface_recognition_model = face_recognition_models.face_recognition_model_location()\r\nface_encoder = dlib.face_recognition_model_v1(face_recognition_model)\r\n\r\n\r\ndef _rect_to_css(rect):\r\n    \"\"\"\r\n    Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order\r\n\r\n    :param rect: a dlib 'rect' object\r\n    :return: a plain tuple representation of the rect in (top, right, bottom, left) order\r\n    \"\"\"\r\n    return rect.top(), rect.right(), rect.bottom(), rect.left()\r\n\r\n\r\ndef _css_to_rect(css):\r\n    \"\"\"\r\n    Convert a tuple in (top, right, bottom, left) order to a dlib `rect` object\r\n\r\n    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\r\n    :return: a dlib `rect` object\r\n    \"\"\"\r\n    return dlib.rectangle(css[3], css[0], css[1], css[2])\r\n\r\n\r\ndef _trim_css_to_bounds(css, image_shape):\r\n    \"\"\"\r\n    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\r\n\r\n    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\r\n    :param image_shape: numpy shape of the image array\r\n    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order\r\n    \"\"\"\r\n    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)\r\n\r\n\r\ndef face_distance(face_encodings, face_to_compare):\r\n    \"\"\"\r\n    Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\r\n    for each comparison face. The distance tells you how similar the faces are.\r\n\r\n    :param faces: List of face encodings to compare\r\n    :param face_to_compare: A face encoding to compare against\r\n    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array\r\n    \"\"\"\r\n    if len(face_encodings) == 0:\r\n        return np.empty((0))\r\n\r\n    return np.linalg.norm(face_encodings - face_to_compare, axis=1)\r\n\r\n\r\ndef load_image_file(file, mode='RGB'):\r\n    \"\"\"\r\n    Loads an image file (.jpg, .png, etc) into a numpy array\r\n\r\n    :param file: image file name or file object to load\r\n    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.\r\n    :return: image contents as numpy array\r\n    \"\"\"\r\n    im = PIL.Image.open(file)\r\n    if mode:\r\n        im = im.convert(mode)\r\n    return np.array(im)\r\n\r\n\r\ndef _raw_face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\r\n    \"\"\"\r\n    Returns an array of bounding boxes of human faces in a image\r\n\r\n    :param img: An image (as a numpy array)\r\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\r\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\r\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\r\n    :return: A list of dlib 'rect' objects of found face locations\r\n    \"\"\"\r\n    if model == \"cnn\":\r\n        return cnn_face_detector(img, number_of_times_to_upsample)\r\n    else:\r\n        return face_detector(img, number_of_times_to_upsample)\r\n\r\n\r\ndef face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\r\n    \"\"\"\r\n    Returns an array of bounding boxes of human faces in a image\r\n\r\n    :param img: An image (as a numpy array)\r\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\r\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\r\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\r\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\r\n    \"\"\"\r\n    if model == \"cnn\":\r\n        return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, \"cnn\")]\r\n    else:\r\n        return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]\r\n\r\n\r\ndef _raw_face_locations_batched(images, number_of_times_to_upsample=1, batch_size=128):\r\n    \"\"\"\r\n    Returns an 2d array of dlib rects of human faces in a image using the cnn face detector\r\n\r\n    :param img: A list of images (each as a numpy array)\r\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\r\n    :return: A list of dlib 'rect' objects of found face locations\r\n    \"\"\"\r\n    return cnn_face_detector(images, number_of_times_to_upsample, batch_size=batch_size)\r\n\r\n\r\ndef batch_face_locations(images, number_of_times_to_upsample=1, batch_size=128):\r\n    \"\"\"\r\n    Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector\r\n    If you are using a GPU, this can give you much faster results since the GPU\r\n    can process batches of images at once. If you aren't using a GPU, you don't need this function.\r\n\r\n    :param images: A list of images (each as a numpy array)\r\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\r\n    :param batch_size: How many images to include in each GPU processing batch.\r\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\r\n    \"\"\"\r\n    def convert_cnn_detections_to_css(detections):\r\n        return [_trim_css_to_bounds(_rect_to_css(face.rect), images[0].shape) for face in detections]\r\n\r\n    raw_detections_batched = _raw_face_locations_batched(images, number_of_times_to_upsample, batch_size)\r\n\r\n    return list(map(convert_cnn_detections_to_css, raw_detections_batched))\r\n\r\n\r\ndef _raw_face_landmarks(face_image, face_locations=None, model=\"large\"):\r\n    if face_locations is None:\r\n        face_locations = _raw_face_locations(face_image)\r\n    else:\r\n        face_locations = [_css_to_rect(face_location) for face_location in face_locations]\r\n\r\n    pose_predictor = pose_predictor_68_point\r\n\r\n    if model == \"small\":\r\n        pose_predictor = pose_predictor_5_point\r\n\r\n    return [pose_predictor(face_image, face_location) for face_location in face_locations]\r\n\r\n\r\ndef face_landmarks(face_image, face_locations=None, model=\"large\"):\r\n    \"\"\"\r\n    Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image\r\n\r\n    :param face_image: image to search\r\n    :param face_locations: Optionally provide a list of face locations to check.\r\n    :param model: Optional - which model to use. \"large\" (default) or \"small\" which only returns 5 points but is faster.\r\n    :return: A list of dicts of face feature locations (eyes, nose, etc)\r\n    \"\"\"\r\n    landmarks = _raw_face_landmarks(face_image, face_locations, model)\r\n    landmarks_as_tuples = [[(p.x, p.y) for p in landmark.parts()] for landmark in landmarks]\r\n\r\n    # For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png\r\n    if model == 'large':\r\n        landmarks = [{\r\n            \"chin\": points[0:17],\r\n            \"left_eyebrow\": points[17:22],\r\n            \"right_eyebrow\": points[22:27],\r\n            \"nose_bridge\": points[27:31],\r\n            \"nose_tip\": points[31:36],\r\n            \"left_eye\": points[36:42],\r\n            \"right_eye\": points[42:48],\r\n            \"top_lip\": points[48:55] + [points[64]] + [points[63]] + [points[62]] + [points[61]] + [points[60]],\r\n            \"bottom_lip\": points[54:60] + [points[48]] + [points[60]] + [points[67]] + [points[66]] + [points[65]] + [points[64]]\r\n        } for points in landmarks_as_tuples]\r\n        \r\n        for landmark in landmarks:\r\n            points = landmark['top_lip']\r\n            # Dapatkan jarak antara kedua mata\r\n            jarak_mata = math.sqrt((landmark['left_eye'][1][0] - landmark['right_eye'][0][0])**2 + (landmark['left_eye'][1][1] - landmark['right_eye'][0][1])**2)\r\n\r\n            # Sesuaikan titik-titik untuk bibir bagian atas\r\n            points[0][0] = landmark['nose_tip'][0][0] - jarak_mata * 0.1\r\n            points[0][1] = landmark['nose_tip'][0][1] - jarak_mata * 0.1\r\n\r\n            # Sesuaikan titik-titik untuk bibir bagian bawah\r\n            points[6][0] = landmark['nose_tip'][0][0] + jarak_mata * 0.1\r\n            points[6][1] = landmark['nose_tip'][0][1] + jarak_mata * 0.1\r\n    elif model == 'small':\r\n        landmarks = [{\r\n            \"nose_tip\": [points[4]],\r\n            \"left_eye\": points[2:4],\r\n            \"right_eye\": points[0:2],\r\n        } for points in landmarks_as_tuples]\r\n    else:\r\n        raise ValueError(\"Invalid landmarks model type. Supported models are ['small', 'large'].\")\r\n\r\n\r\n\r\ndef face_encodings(face_image, known_face_locations=None, num_jitters=2, model=\"large\"):\r\n    \"\"\"\r\n    Given an image, return the 128-dimension face encoding for each face in the image.\r\n\r\n    :param face_image: The image that contains one or more faces\r\n    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.\r\n    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)\r\n    :param model: Optional - which model to use. \"large\" (default) or \"small\" which only returns 5 points but is faster.\r\n    :return: A list of 128-dimensional face encodings (one for each face in the image)\r\n    \"\"\"\r\n    raw_landmarks = _raw_face_landmarks(face_image, known_face_locations, model)\r\n    return [np.array(face_encoder.compute_face_descriptor(face_image, raw_landmark_set, num_jitters)) for raw_landmark_set in raw_landmarks]\r\n\r\n\r\ndef compare_faces(known_face_encodings, face_encoding_to_check, tolerance=0.6):\r\n    \"\"\"\r\n    Compare a list of face encodings against a candidate encoding to see if they match.\r\n\r\n    :param known_face_encodings: A list of known face encodings\r\n    :param face_encoding_to_check: A single face encoding to compare against the list\r\n    :param tolerance: How much distance between faces to consider it a match. Lower is more strict. 0.6 is typical best performance.\r\n    :return: A list of True/False values indicating which known_face_encodings match the face encoding to check\r\n    \"\"\"\r\n    return list(face_distance(known_face_encodings, face_encoding_to_check) <= tolerance)\r\n"
        }
    ]
}